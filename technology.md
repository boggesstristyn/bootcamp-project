# Technologies Used
## Data Cleaning and Analysis
Our initial data sources are:
- Austin Police Department Crime Data CSV: https://data.austintexas.gov/Public-Safety/Crime-Reports/fdj4-gpfu
- Austin Housing Market Analysis Data by Zip Code CSV: https://data.austintexas.gov/Housing-and-Real-Estate/2014-Housing-Market-Analysis-Data-by-Zip-Code/hcnj-rei3
- City of Austin Housing and Planning Anti-Displacement Data CSV: https://data.austintexas.gov/Housing-and-Real-Estate/Project-Connect-Anti-Displacement-Dashboard-Data-2/e2tx-ut3v*

To explore and clean our data, we will use Python and the Pandas library. The crime dataset will likely require the most cleaning, as not all of the column variables will be necessary for our analysis and there are numerous incidents (rows) that have null values. We will most likely lose a significant amount of data with our cleaning, but with over 2.39 million incident entries, we may still need to apply a filter on the results to only include the most recent year’s data in our analysis. The remaining datasets will likely require more cleaning in regards to columns that are not needed for our project and finding common variables to link the multiple datasets by for our database creation. We intend to use the Displacement Risk Demographics dataset for our machine learning algorithm, so we will need to determine which variables will be the most helpful for our model. 

*We originally began our project with the 2019 Displacement Risk Demographics dataset, but data.austintexas.gov has since updated this dataset to a 2020 version. This link has been provided, as the 2019 has been deleted, but we are leaning towards using 2019's dataset as the columns were more discriptive and the dataset is cleaner. 

## Database Storage
We intend to use MongoDB as our database. We have manually saved the Austin Housing Market Analysis, Displacement Risk Areas, and Displacement Risk Demographics csv files, but as the APD Crime dataset is updated daily and contains over 2 million unique rows, we choose to use an API for this source. The database will connect to the data.austin.gov API and filter for incidents with occurrence dates later than 01/01/2014 and limiting the results to 10,000. If we find that our machine learning model could benefit from more data, we can easily edit these parameters. Using MongoDB will allow our database more flexibility compared to a SQL or SQLite database. Flask-pyMongo will be used to bridge Flask and MongoDB.

## Machine Learning
To create our machine learning algorithm, we intend to use Python and the NumPy, SciPy, Scikit-learn, and Matplotlib.pylot libraries. NumPy will help us as we are working with large datasets/arrays that we may need to perform functions or transformations on. Scikit-learn will be the most essential to this component of the project, as it allows us access to supervised/unsupervised learning algorithms. We intend to use a Random Forest Model, as we believe it will be beneficial to rank our features by importance. We can test different rankings and see which model produces the best results. SciPy will help with any statistics, optimization, and manipulations that are often used in the preprocessing step of machine learning algorithms. Matplotlib.pylot will be helpful for any visualizations that we choose to create within Python. 

## Dashboard
We intend to use Flask and Bootstrap’s features to build our dashboard. Since a couple of our datasets include longitude/latitude/zip code variables, we would like to build an interactive map that would allow the user to either enter their desired filter features and highlights on the map the areas that meet those filter specifications, or a map that allows the client to click on zip codes and presents them with a pop-up with information pertaining to that zip code (ex: median household income, population below poverty level, household size, crime rate).  We also intend to use CSS to customize and D3 to add interactivity. We will also create visualizations with Tableau and Matplotlib.pylot to enhance our dashboard. The dashboard will be hosted on GitHub Pages.